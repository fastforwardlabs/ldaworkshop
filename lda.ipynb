{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text\n",
    "\n",
    "There are lots of things you might want to do with documents\n",
    " - group them together\n",
    " - summarize one or several of them\n",
    " - classify them (e.g. sentiment, tagging)\n",
    " - explore, e.g. temporal trends in what they discuss\n",
    "\n",
    "To do any of these things using a computer, you first need to convert words into numbers.\n",
    "\n",
    "[https://github.com/fastforwardlabs/usaa-lda](https://github.com/fastforwardlabs/usaa-lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bags of words\n",
    "\n",
    "The \"bag of words\" approach is the simplest method for this.\n",
    "\n",
    "![Bag of words](img/bagwords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are at least three problems with this approach:\n",
    "\n",
    " - **long, sparse data**: a short text is turned into a very long series of numbers, most of which are zero (\"sparse\").\n",
    " \n",
    " - **synonyms and multiple meanings**: \"movies\" and \"films\" are different (they should not be), and \"bow\" (that shoots an arrow) and \"bow\" (you make to a King) are not (they should be)\n",
    " \n",
    " - **word order**: totally lost (\"man bites dog\" and \"dog bites man\" are the same).\n",
    "\n",
    "We're going to look at a technique that addresses the first of these two problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic modeling\n",
    "\n",
    "Topic modeling is a statistical method to find groups of words that tend to co-occur in a corpus of documents.\n",
    "\n",
    "For example, maybe the words \"movie\", \"film\" and \"director\" often occur in the same documents. That would make them a \"topic\".\n",
    "\n",
    "Topic modeling algorithms find these groups automatically. They are an instance of the class of algorithms known as \"unsupervised machine learning\".\n",
    "\n",
    "In doing this, we become able to express documents as a combination of a relatively small number (~100) of topics, rather than thousands of words (most of which don't occur), and we can treat documents about \"films\" and \"movies\" similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic modeling workflow\n",
    "\n",
    "Topic modeling has two steps:\n",
    "\n",
    " - learn topics from a corpus of representative documents\n",
    " - figure out which of these topics occur in the particular document(s) you're interested in\n",
    " \n",
    "If you're a machine learning person, you'll recognize these as training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lda_topics.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lda_evaluate.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once you've done the second step, you've expressed your new document as a short vector of numbers that you can no do all sorts of things with:\n",
    "\n",
    " - group documents together\n",
    " - summarize one or several documents\n",
    " - classify it (e.g. sentiment, tagging)\n",
    " - explore, e.g. temporal trends in what people are discussing, e.g. [Time-series plots of 1000 topics extracted from 20 years of the New York Times.](http://christo.cs.umass.edu/NYT/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "The best known and best algorithm for finding topics in a corpus is Latent Dirichlet Allocation. It's got a complicated name and, to be frank, it's a complicated algorithm. If you'd like to begin to dig into the details, there are two resources I recommend very highly!\n",
    "\n",
    " - [Tim Hopper's PyData NYC 2015 talk](https://www.youtube.com/watch?v=_R66X_udxZQ)\n",
    " - [David Blei's ACM article](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)\n",
    " \n",
    "For the purposes of this talk, I'm just going to say it finds groups of words that co-occur by magic!\n",
    "\n",
    "The good news is, there are several excellent open source implementations of the algorithm, and we're going to use one of those today.\n",
    "\n",
    "We're going to apply it to a public dataset of Amazon product reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Load data\n",
    "\n",
    "The cell below opens the file and loads it into a pandas dataframe.\n",
    "\n",
    "There's a lot going on in this line, all of which is useful to understand if you're a Python programmer, but none of which is necessary to understand if you're only interested in LDA.\n",
    "\n",
    "If you would like a more detailed explanation of what's going on, please see [the Data notebook](data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"reviews.json\", 'rb') as f:\n",
    "    reviews = pd.DataFrame(json.loads(l.decode()) for l in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pandas dataframe is a structured table-like object that, among many other things, supports a bunch of SQL-like operations and handles fiddly data types like data and times well. I don't know much about R, but I understand R has objects like this too.\n",
    "\n",
    "`head` allows us to see the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual columns can be accessed as keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews['asin'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{} reviews\".format(len(reviews)))\n",
    "print(\"of {} products \".format(len(reviews.asin.unique())))\n",
    "print(\"by {} unique authors \".format(len(reviews.reviewerID.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = reviews.loc[:19999, 'reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize reviews\n",
    "\n",
    "You have to preprocess the text a little before you apply LDA: you need to split documents into words, and you need to turn words into vectorized numbers.\n",
    "\n",
    "Ironically, in order to get the benefits of LDA, we first need to run bag of words on our data!\n",
    "\n",
    "The code to do this is built into scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = LatentDirichletAllocation(n_topics=100, learning_method='batch', n_jobs=-2, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you should do when you fit a topic model is inspect a few of the words that dominate each topic to check that the topics are coherent.\n",
    "\n",
    "To do this, we need to look at the `components_` attribute now attached to `lda`. This is an array with `n_topics` rows and a number of columns equal to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in this array is the weight of the corresponding word in the corresponding topic.\n",
    "\n",
    "The weights of each topic should add up to one, i.e. each row of `lda.components_` should add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear! It turns out there's a bug in scikit-learn's implementation of LDA. Let's fix it here. This should be fixed in the next version of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array of topics and words (or terms) is usually called the topic-term matrix, so let's save it under that name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_term = lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word corresponding to each column in this array, which we'll call the `vocabulary`, is available as a list from `vectorizer.get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.get_feature_names()\n",
    "print(vocabulary[:10000:1000]) # print the 0th, 999th, 1999th, 2999th, etc. item in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the top 10 words that dominate each topic.\n",
    "\n",
    "It does that by going through each row (i.e. each topic) in the `topic_term` array, finding the biggest numbers in that row, then finding the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_topic(topic_term, topic_id, vocabulary):\n",
    "    print(\"{:2d} \".format(topic_id) + \" \".join(vocabulary[i] for i in topic_term[i].argsort()[:-11:-1]))\n",
    "\n",
    "for i, _  in enumerate(topic_term):\n",
    "    print_topic(topic_term, i, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the topics are reasonably coherent, so I'm going to move on.\n",
    "\n",
    "If things look messy:\n",
    " - n_topics might be too large, either for the diversity in the corpus (maybe there really aren't 1000 topics), or for the number of documents you have (you just don't have enough data)\n",
    " - n_topics might be too low (real topics have to be merged together by the algorithm, which doesn't work well)\n",
    " - you've got a bug!\n",
    " \n",
    "Setting n_topics very small (say 5) or very high (say 1000) is a good way of building up some intuition for what works (although beware `n_topics=1000` will take a long time to run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect a document in the corpus\n",
    "\n",
    "The topic model is the lens through which we're going to view future documents.\n",
    "\n",
    "But let's first look at our existing documents through this lens.\n",
    "\n",
    "To do that we have to transform the documents we trained on to be distributions of topics (e.g. document 1 is 20% topic A, 30% topic B, etc.)\n",
    "\n",
    "We do that by running the `lda.transform` method on the vectorized documents `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_topic` has a row for each document, and a column for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, each row should add up to 1, and again they don't because of a bug in scikit-learn, so let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic /= doc_topic.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's look at the topic distribution of a random document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.loc[8, 'reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_topics = (doc_topic[7]).argsort()[:-6:-1]\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in top_topics:\n",
    "    print_topic(topic_term, i, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis is a comprehensive package for visualizing the results of a topic model. It's useful for understanding the structure of the model you've just discovered. The topics exist in a huge space. This package squeezes things down to 2D so we can look at it on the screen.\n",
    "\n",
    "In my experience, it generates a ton of spurious warnings, so let's disable warnings for this package when we import it (a useful trick!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    try:\n",
    "        import pyLDAvis\n",
    "    except ImportError:\n",
    "        print('ERROR: pyLDAvis not installed! Skip to next section!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `topic_term` and `doc_topic` matrices, pyLDAvis needs to know how often each word occurs in the entire corpus, and how long each document is. Here are calculations that give those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_frequency = np.asarray(X.sum(axis=0)).squeeze()\n",
    "doc_lengths = [len(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.prepare(topic_term_dists=topic_term,\n",
    "                           doc_topic_dists=doc_topic,\n",
    "                           doc_lengths=doc_lengths,\n",
    "                           vocab=vocabulary,\n",
    "                           term_frequency=term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all this together in a Pipeline and persist the model\n",
    "\n",
    "The process of getting from document to topic distribution is a little fiddly. We need to:\n",
    " - Vectorize the document (using the same vocabulary we used when training above)\n",
    " - Transform the document using the LDA object\n",
    " \n",
    "scikit-learn allows us to bundle these steps (and more!) together in an object called a `Pipeline`, which we can save to disk, reload, and work with again. Let's build one, train it, and save it.\n",
    "\n",
    "**WARNING**: this next cell will take a while to execute the first time you run it. After that though, the model will be loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "try:\n",
    "    with open('topic_model.pkl', 'rb') as f:\n",
    "        topic_pipeline = pickle.load(f)\n",
    "    pipeline_vocabulary = topic_pipeline.steps[0][1].get_feature_names()\n",
    "except IOError:\n",
    "    topic_pipeline = make_pipeline(\n",
    "        CountVectorizer(stop_words='english', max_features=10000),\n",
    "        LatentDirichletAllocation(n_topics=100, learning_method='batch', n_jobs=-2, random_state=0)\n",
    "    )\n",
    "    topic_pipeline.fit(texts)\n",
    "    with open('topic_model.pkl', 'wb') as f:\n",
    "        pickle.dump(topic_pipeline, f)\n",
    "        \n",
    "pipeline_vocabulary = topic_pipeline.steps[0][1].get_feature_names()\n",
    "pipeline_topic_term = topic_pipeline.steps[1][1].components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine topics of a new document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single document we looks at above was a pretty short document. Let's make a more interesting, longer document out of all the reviews of that product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomreviews = \" \".join(texts[1100:1105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(randomreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = topic_pipeline.transform([randomreviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_topics = (doc_topic[0]).argsort()[:-6:-1]\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in top_topics:\n",
    "    print_topic(pipeline_topic_term, i, pipeline_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What next?\n",
    "\n",
    "Use the `doc_topic` array for a downstream task, e.g.\n",
    " - corpus exploration (remember the [NYT visualization](http://christo.cs.umass.edu/NYT/))\n",
    " - document clustering, e.g. use something like `KMeans` (in scikit-learn) to visualize which documents are most similar in terms of their topics, which may surface groups of topics or groups of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarization\n",
    "\n",
    "Here's a short algorithm, but see [Fast Forward Labs Report 04](http://ff04.fastforwardlabs.com) for details:\n",
    "  - Train LDA on all products of a certain type (e.g. all the books)\n",
    "  - Treat all the reviews of a particular product as one document, and infer their topic distribution\n",
    "  - Infer the topic distribution for each sentence\n",
    "  - For each topic that dominates the reviews of a product, pick some sentences that are themselves dominated by that topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/strain.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Be aware of limitations:\n",
    " - Choosing `n_topics` is an art rather than a science!\n",
    " - The topics don't come with names. Sometimes they overlap. Sometimes they're not what you want them to be. For example, if you run a topic model on the NYT corpus, there's no guarantee you'll get topics that correspond to the sections of the newspaper (business, metro, world, sport, etc.!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The way you used `fit` and `transform` for both the `vectorizer`, `lda`, and `topic_pipeline` objects is generic across scikit-learn, so play with scikit-learn, e.g. [Andreas Mueller's presentation](https://www.youtube.com/watch?v=8CzwlZbwDkI) is a good place to start.\n",
    " \n",
    "Remember if you're interested in the LDA algorithm itself, take a look at\n",
    "\n",
    " - [Tim Hopper's PyData NYC 2015 talk](https://www.youtube.com/watch?v=_R66X_udxZQ)\n",
    " - [David Blei's ACM article](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
